\documentclass[]{article}

\usepackage{xcolor}
\usepackage{amsmath,amssymb,mathtools} % maths symbols 
\usepackage{amsthm} % define proof environment
\usepackage{amsfonts} % extended maths symbols and fonts 
%\usepackage{algorithm2e}

\newtheorem{theorem}{Theorem}[section] 
\newtheorem{lemma}{Lemma}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{definition}{Definition}[section] % definition numbers are dependent on theorem numbers
\newtheorem{proposition}{Proposition}[section] % same for example numbers

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%opening
\title{Spectral Graph Theory}
\author{Kelvin Li}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		These are the study notes of the course \textit{Spectral Graph Theory} by Daniel Spielman at the Yale University, Fall 2015. There are 26 lectures. I try to summarise the important results and topics in each of the lectures and my own interpretations of these results. 
		
		A reference book that I use is \textit{Spectra of Graphs} by Andries E. Brouwer and Willem H. Haemers, 2010. 
	\end{abstract}
	
	\section{Lecture 1 Introduction}
	Spectral graph theory has applications in graph drawing, graph isomorphism and etc.
	
	There are two natural matrices corresponding to a graph $G(V,E)$, the adjacency matrix $A$ and the degree (or diagonal) matrix $D$. These two matrices themselves are boring, but some interesting matrices are defined based on them. For example, we can define the \textit{diffusion matrix} as
	\begin{align*}
	W = D^{-1}A
	\end{align*} 
	that describes the diffusion of stuff (e.g., gas) among the vertices of a graph. We can also define an important matrix called the the \textit{Laplacian matrix} 
	\begin{align*}
	L=D-A.
	\end{align*}
	Given a function (or vector) $x \in \mathbb{R}^V$ on the vertices, the \textit{Laplacian quadratic form} is 
	\begin{align*}
	x^T L x = \sum_{(u,v)\in E} (x(u) - x(v))^2.
	\end{align*}
	It measures the (smoothness?) of $x$. 
	
	\begin{definition}
		A matrix $A$ is \textbf{similar} to a matrix $B$ if there is a non-singular\footnote{A matrix is \textbf{singular} if and only its determinant is zero.} matrix $M$ such that $M^{-1}AM=B$. 
	\end{definition}
	If $A$ and $B$ are similar, then they have the same set of eigenvalues. Almost all the matrices consider in this course are \textit{symmetric} or similar to symmetric matrices. 
	
	\begin{definition}
		Let $M$ be an $n\times n$ matrix. The \textbf{characteristic polynomial} of $M$ is $p_M(x)=\det (xI - M)$.
	\end{definition}
	The \textbf{eigenvalues} of $M$ are the roots of the equation $p_M(x)=0$.
	
	\begin{definition}
		A vector $\psi$ is an \textbf{eigenvector} of a matrix $M$ with eigenvalue $\lambda$ if $M\psi=\lambda \psi$.
	\end{definition}
	A matrix represents a linear transformation. An eigenvector of a matrix $M$ is a non-zero vector that is invariant under the linear transformation by $M$, up to a scalar. This scalar is the eigenvalue, to which the eigenvector correspond. In other words, under the linear transformation by $M$, the eigenvectors do not change directions, but their scale may or may not change, depending on their corresponding eigenvalues. If an eigenvector is for the eigenvalue $1$, then its magnitude does not change under $M$. If its eigenvalue is $3$, its magnitude is scaled 3 times. 
	
	The eigenvectors of a given eigenvalue are only determinant up to an orthogonal transformation. 
	
	\begin{theorem} (\textbf{Spectral theorem})
		An $n \times n$ real symmetric matrix has $n$ eigenvalues and $n$ eigenvectors that are unit vectors and mutually orthogonal.
	\end{theorem}
	It a matrix is not symmetric, it may not have $n$ eigenvalues. Even if it has $n$ eigenvalues, the corresponding $n$ eigenvectors will not be mutually orthogonal. 
	
	\begin{definition}
		The \textbf{Rayleigh quotient} of a vector $x$ w.r.t. a matrix $M$ is the ratio 
		\begin{align*}
		\frac{x^T M x}{x^T x}. 
		\end{align*}
		That is, 
		\begin{align*}
		\psi = \arg\max_{x} \frac{x^T M x}{x^T x}
		\end{align*}
		is an eigenvector of $M$ for the eigenvalue $\lambda$, where 
		\begin{align*}
		\lambda = \max_{x} \frac{x^T M x}{x^T x}.
		\end{align*}
	\end{definition}
	By definition, if $\psi$ is a eigenvector of $M$ for the eigenvalue $\lambda$, then the Rayleigh quotient of $\psi$ is $\lambda$. The notion of Rayleigh quotient is important, because it allows one to prove more insights about the largest (or smallest) eigenvalue of a matrix. 
	
	\begin{theorem} (\textbf{Rayleigh quotient maximization})
		Let $M$ be a symmetric matrix and $x$ be a non-zero vector that maximizes the Rayleigh quotient w.r.t. $M$. Then $x$ is an eigenvector of $M$ w.r.t. the eigenvalue that is equal to the Rayleigh quotient, and this eigenvalue is the largest eigenvalue of $M$. 
	\end{theorem}
	The theorem can also be proved for \textbf{minimization}, in which the Rayleigh quotient is the smallest eigenvalue of $M$. 
	
	\begin{lemma} (\textbf{Smallest eigenvalue of Laplacian})
		\label{lemma:smallest eigenvalue of laplacian}
		The smallest eigenvalue of the Laplacian is zero. The corresponding eigenvector is a constant vector $x=(c,\dots, c)$.
	\end{lemma}
	The lemma makes sense intuitively. The sum of each column (or row) of the Laplacian is 0, so for the constant vector $\vec{1} = (1, \dots, 1)$, we have $0 = L \vec{1} = \lambda \vec{1}$. Hence, $\lambda=0$. Since the determinant of a matrix is the product of all its eigenvalues, this also implies that the \textbf{Laplacian matrix is singular}. 
	\begin{proof}
		Using Rayleigh quotient, in which $M$ is the graph Laplacian, so the numerator is just the Laplacian quadratic form, in which each term in the summation is non-negative. So the smallest numerator is $0$, which is obtained when $x$ is a constant vector (or function). By the above theorem, this minimized Rayleigh quotient is the smallest eigenvalue of $M$. 
	\end{proof}
	
	Assume without loss of generality that the eigenvalues of $M$ are sorted $\lambda_1 \le \lambda_2 \le \dots \le \lambda_n$. 
	
	\begin{lemma} (\textbf{Second smallest eigenvalue of Laplacian})
		$\lambda_2 > 0$ if and only if the graph $G$ is connected. 
	\end{lemma}
	\begin{proof}
		Skipped.
	\end{proof}
	
	The second smallest eigenvalue $\lambda_2$ of a graph Laplacian is called the \textbf{algebraic connectivity of a graph}. Later, we will see that the smallest eigenvalue of the diffusion matrix is zero if and only if the graph is bipartite. 
	
	The $i^{th}$ eigenvector of the graph Laplacian is (approximately?) a polynomial of order $i-1$ of node index. That is, when plotting the second eigenvector (on the Y-axis) against node indices (on the X-index), the plot is roughly a straight line. When plotting the third, the plot is a quadratic curve. See plots near the end of Lecture 1. 
	
	\subsection{Notes}
	Some additional nodes worth mentioning: 
	
	\begin{enumerate}
		\item By Spectral theorem, the adjacency and Laplacian matrices' eigenvectors form $n$ orthonormal basis. Hence, their eigenvalues' \textbf{algebraic} and \textbf{geometric multiplicities} are identical. Hence, we use the term multiplicity without referencing to algebraic or geometric. 
		\item The \textbf{(ordinary) spectrum} of a graph is the spectrum of the graph's adjacency matrix, that is, its sets of eigenvalues and multiplicities. The \textbf{Laplace spectrum} is the spectrum of the Laplacian matrix. 
		\item The eigenvalues and eigenvectors are of a adjacency or Laplacian matrix are \textbf{invariant under vertex numbering}. 
	\end{enumerate}
	
	\section{Lecture 2 The Laplacian}
	
	\begin{theorem} (\textbf{Eigenvalues and eigenvectors optimization})
		Let $M$ be an $n \times n$ symmetric matrix with eigenvalues $\lambda_1 \le \lambda_2 \le \dots \lambda_n$ with corresponding eigenvectors $\psi_1, \dots, \psi_n$. Then 
		\begin{align*}
		\lambda_i &= \min_{x \perp \psi_1, \dots, \psi_{i-1}} \frac{x^T M x}{x^T x}, \\
		\psi_i &= \arg\min_{x \perp \psi_1, \dots, \psi_{i-1}} \frac{x^T M x}{x^T x}.
		\end{align*}
	\end{theorem}
	
	The \textbf{Laplacian quadratic form} for a weighted graph $G=(V,E)$ is 
	\begin{align}
	\label{equation:laplacian quadratic form}
	x^T L x = \sum_{(uv) \in E} w_{uv} (x(u) - x(v))^2.
	\end{align}
	
	Let $L_{G_{uv}}$ be the Laplacian of the graph $G_{uv}$, in which the graphs has only one edge $E(G_{uv})=\{uv\}$ and possibly more than two vertices. Hence, $L_{G_{uv}}$ has the form that the intersections between the rows and columns indexed by $u$ and $v$ are $\begin{bmatrix}
	1 & -1 \\
	-1 & 1
	\end{bmatrix}$ and everywhere else is $0$. Hence, the Laplacian of a graph $G$ can be obtained by summing $L_{G_{uv}}$ over all edges $uv$ in $G$. That is, 
	\begin{align*}
	L_G = \sum_{uv \in E} w_{uv} L_{G_{uv}}.
	\end{align*}
	Intuitively this makes sense too, because if the vertex $u$ has degree $d(u)=3$, the corresponding row and column for the vertex $u$ will appear $3$ times in the summation. 
	
	Another important interpretation of the Laplacian matrix is that it is an operator 
	\begin{align*}
	(L_Gx)(u) = d(u)x(u) - \sum_{uv \in E}w_{uv}x(v) = \sum_{uv \in E} w_{uv}(x(u)-x(v)).
	\end{align*}
	
	\subsection{Graph drawing}
	When drawing a graph, take a vector $x \in \mathbb{R}^V$ that maps each vertex of the graph $G$ to a real value. We would like two adjacent vertices to have close values. \footnote{Probably because we do not want an edge to cross half of the space to connect two vertices.} So this becomes a constraint optimization problem. That is, find a vector $x$ such that Equation (\ref{equation:laplacian quadratic form}) is minimized, where the constraints are $||x||^2=1$ to avoid $x$ be a zero vector and $\sum_{i=1}^V x(i) = 0$ to avoid $x$ being a constant vector with $x(i)=1/\sqrt{n}$.
	
	When drawing the graph in one dimension, the optimal $x$ is the eigenvector of norm $1$ for the second smallest eigenvalue $\lambda_2$. When drawing in two dimensions, we need to find two optimal vectors $x,y$ with ad additional constraint that they are orthogonal. So the optimal $x,y$ are the unit eigenvectors for $\lambda_2,\lambda_3$ respectively. 
	
	\subsection{Isoperimetry, expander graph and $\lambda_2$}
	\begin{definition}
		Let $S \subseteq V$ be a subset of vertices of the graph $G=(V,E)$. The \textbf{boundary} of $S$ is 
		\begin{align*}
		\partial(S) = \{uv \in E \mid u \in S, v \notin S\}
		\end{align*}
		the set of edges connecting $S$ and the rest of the graph. 
	\end{definition}
	
	\begin{definition}
		Let $S \subseteq V$ be a non-empty subset of vertices of the graph $G=(V,E)$. The \textbf{isoperimetric ratio} of $S$ is 
		\begin{align*}
		\theta(S) = \frac{|\partial(S)|}{|S|}.
		\end{align*}
	\end{definition}
	
	\begin{definition}
		The \textbf{isoperimetric number (or edge expansion or Cheeger constant)} of a graph $G$ with $n$ vertices is  
		\begin{align*}
		\theta_G = \min_{S:|S|\le n/2} \theta(S).
		\end{align*}
		the minimum isoperimetric ratio over all non-empty subsets of at most half the vertices in $G$.
	\end{definition}
	
	Based on these definitions, we can prove the next theorem, which gives a \textbf{lower bound} on the isoperimetric number of a graph $G$ in terms of the second smallest eigenvalue $\lambda_2$ of the graph Laplacian. 
	\begin{theorem}
		Let $G=(V,E)$ be a graph. For every $S \subsetneq V$, the isoperimetric ratio of $S$ satisfies 
		\begin{align*}
		\theta(S) \ge \lambda_2(1-s),
		\end{align*}
		where $s=|S|/|V|$ and $\lambda_2$ is the second smallest eigenvalue of the graph Laplacian. In particular, the isoperimetric number of $G$ satisfies 
		\begin{align*}
		\theta_G \ge \frac{\lambda_2}{2}.
		\end{align*}
	\end{theorem}
	\begin{proof}
		The proof relies on the \textbf{characteristic vector (or indicator vector)} of the set $S$ 
		\begin{align*}
		\chi_S(u) = 
		\begin{cases}
		1 & \quad \text{ if } u \in S\\
		0 & \quad \text{ otherwise.}
		\end{cases}
		\end{align*}
		It shows that the characteristic vector is not orthogonal to the $\psi_1$ that is a constant vector. But it can be used to create another vector $x$ that is orthogonal to $\psi_1$. 
	\end{proof}
	The theorems says that if $\lambda_2$ is large, then the graph $G$ is very well connected, because the boundary of any small subset of vertices is large. This relates to the concept of \textbf{expander graph}, which is vaguely understood as a finite undirected graph in which every small subset of vertices has large boundary. Every connected graph is an expander graph. A disconnect graph is not an expander graph, because there exists a component which has empty boundary. 
	
	\begin{remark}
		Let $S \subseteq V$ and $s = |S|/|V|$. Then 
		\begin{align*}
		||\chi_S - s \vec{1}||^2 = s(1-s)|V|.
		\end{align*}
	\end{remark}
	
	\subsection{Eigenvalues and eigenvectors of the Laplacian of some graphs}
	
	For complete graphs $K_n$. 
	
	\begin{theorem}
		The Laplacian of the complete graph $K_n$ has eigenvalue $0$ with multiplicity $1$ and $n$ with multiplicity $n-1$.
	\end{theorem}
	
	For start graph $S_n$. 
	
	\begin{lemma}
		Let $G=(V,E)$ be a graph, and let $v,w$ be vertices of degree one that are both connected to another vertex $z$. Then, the vector $\psi=\delta_v-\delta_w$ is an eigenvector of $L_G$ of eigenvalue $1$. 
	\end{lemma}
	
	\begin{theorem}
		The graph $S_n$ has eigenvalue $0$ with multiplicity $1$, eigenvalue $1$ with multiplicity $n-2$, and eigenvalue $n$ with multiplicity $1$. 
	\end{theorem}
	
	Hypercube graphs, skipped. 
	
	\section{Lecture 3 The adjacency matrix and graph coloring}
	
	\subsection{Adjacency matrix}
	The adjacency matrix $A$ of a weighted graph can be viewed as an operator that acts on a vector $x \in \mathbb{R}^V$ by 
	\begin{align*}
	(Ax)(u) = \sum_{uv\in E} w_{uv}x(v).
	\end{align*}
	
	Denote the eigenvalues of $A$ by $\mu_1, \dots, \mu_n$ and order them in the opposite direction $\mu_1 \ge \mu_2 \ge \dots \ge \mu_n$. 
	
	If $G$ is a $d$-regular graph, then $L = Id-A$. The eigenvalues satisfy $\lambda_i = d- \mu_i$. The largest $\mu_i=d$, which corresponds to the smallest $\lambda_i=0$. Hence, the eigenvector for the largest eigenvalue of the adjacency matrix is a constant vector. 
	
	\begin{lemma} (\textbf{Bounds on the largest eigenvalue})
		Let $d_{max}$ and $d_{ave}$ be the maximum and average degree of a vertex in a graph $G$. Then 
		\begin{align*}
		d_{ave} \le \mu_1 \le d_{max}.
		\end{align*}
	\end{lemma}
	
	\begin{lemma}(\textbf{Better lower bound})
		For every $S \subseteq V$, let $d_{ave}(S)$ be the average degree of vertices in the induced subgraph $G[S]$ by the vertices in $S$. Then 
		\begin{align*}
		d_{ave}(S) \le \mu_1.
		\end{align*}
	\end{lemma}
	This lemma is proved using the following result from linear algebra. 
	
	\begin{lemma}
		Let $A$ be a symmetric matrix and let $S$ be a subset of its row and columns indices. Then the eigenvalues satisfy  
		\begin{align*}
		\lambda_{max}(A) \ge \lambda_{max}(A(S)) \ge \lambda_{min}(A(S)) \ge \lambda_{min}(A).
		\end{align*}
	\end{lemma}
	
	\begin{lemma} (\textbf{Largest eigenvalue})
		If $G$ is connected and $\mu_1=d_{max}$, then $G$ is $d_{max}$-regular.
	\end{lemma}
	
	Although the largest eigenvalue of the adjacency matrix of a regular graph is 0 and hence its corresponding eigenvector is a constant vector, the eigenvector is usually not constant for general graphs. 
	
	
	\begin{lemma} (\textbf{Non-negative implies positive})
		Let $G$ be a connected (non-negative) weighted graph. If $\phi$ is a non-negative eigenvector of its adjacency matrix $A$, then $\phi$ is strictly positive. 
	\end{lemma}
	
	\begin{theorem} (\textbf{Perron-Frobenius for symmetric graphs})
		Let $G$ be a connected weighted symmetric graph. Let $A$ be its adjacency matrix and $\mu_1 \ge \dots \ge \mu_n$ be its eigenvalues. Then 
		\begin{enumerate}
			\item $\mu_1 \ge -\mu_n$.
			\item $\mu_1 > \mu_2$.
			\item The eigenvalue $\mu_1$ has a strictly positive eigenvector.
		\end{enumerate}
	\end{theorem}
	
	\begin{proof}
		To prove part 3, we can construct a vector $x = |\phi_1|$ that is the absolute value of the eigenvector for the largest eigenvalue $\mu_1$. Since $x$ is non-negative, once we can prove that $x$ is also an eigenvector of $\mu_1$, we can apply the above lemma to conclude that $\mu_1$ has a positive eigenvector. 
	\end{proof}
	
	\begin{proposition} (\textbf{Bipartite graph})
		If $G$ is connected, then $\mu_n=-\mu_1$ if and only if $G$ is bipartite. 
	\end{proposition}
	
	The $n^{th}$ eigenvalue $\mu_n$ (smallest of $A$) and $\lambda_n$ (largest of $L$) correspond to the highest \textbf{frequency vibration}\footnote{What is it?} in a graph. The corresponding eigenvector tries to assign as different as possible values to neighbouring vertices. This is consistent with the plot for the largest eigenvector in Lecture 1, in which at each node index, the plot has a local optimal.\footnote{Because eigenvectors are invariant under vertex numbering.} 
	
	
	\subsection{Graph coloring}
	
	\begin{definition}
		A \textbf{$k$-coloring} of a graph $G=(V,E)$ is a function $c: V \rightarrow \{1, \dots, k\}$ so that for all edges $uv \in E$, we have $c(u) \neq c(v)$. 
	\end{definition}
	
	\begin{definition}
		A graph is \textbf{$k$-colorable} if it has a $k$-coloring. 
	\end{definition}
	
	\begin{definition}
		The \textbf{chromatic number} of a graph $G$, written $\chi_G$, is the least $k$ for which $G$ is $k$-colorable. 
	\end{definition}
	
	\begin{lemma} (\textbf{2-colorable})
		A graph $G$ is 2-colorable if and only if it is bipartite. 
	\end{lemma}
	
	\begin{lemma} (\textbf{3-colorable})
		Determining whether or not a graph is 3-colorable is an NP-complete problem. 
	\end{lemma}
	
	\begin{theorem} (\textbf{4-colorable})
		Every planar graph is 4-colorable. 
	\end{theorem}
	
	\begin{remark}
		Every graph is $d_{max}+1$-colorable.
	\end{remark}
	
	\begin{remark}
		Order the vertices of a graph $1,\dots,n$. Let $k$ be an upper bound on the number of neighbours of every vertex $u \in V$ that has smaller labels. That is, 
		\begin{align*}
		|\{v \mid uv \in E and v < u\}| \le k, \quad \forall u \in V.
		\end{align*}
		Then $G$ can be colored with at most $k+1$ colors. 
	\end{remark}
	
	\begin{theorem} (\textbf{Wilf's theorem})
		Let $\mu_1$ be the largest eigenvalue of the adjacency matrix of a graph $G$. The chromatic number of $G$ satisfies 
		\begin{align*}
		\chi_G \le \floor*{\mu_1} + 1.
		\end{align*}
	\end{theorem}
	\begin{proof}
		The proof relies on the above remark. It first finds a vertex with degree a most $\mu_1$. Such a vertex exists due to the bound that $d_{ave}(G) \le \mu_1$. Label this vertex $n$. Then use the fact that the largest eigenvalue of any subgraph is bounded above by $\mu_1$.
	\end{proof}
	
	
	\begin{lemma} (\textbf{Block matrix})
		Let 
		
		\begin{align*}
		A = 
		\begin{bmatrix}
		A_{1,1} & A_{1,2} & \cdots & A_{1,k} \\
		A_{1,2}^T & A_{2,2} & \cdots & A_{2,k} \\
		\vdots & \vdots & \ddots & \vdots \\
		A_{1,k}^T & A_{2,k}^T & \cdots & A_{k,k}
		\end{bmatrix}
		\end{align*}
		be a block-partitioned symmetric matrix with $k \ge 2$. Then 
		\begin{align*}
		(k-1) \lambda_{min}(A) + \lambda_{max}(A) \le \sum_i \lambda_{max}(A_{i,i}).
		\end{align*}
	\end{lemma}
	
	
	\begin{theorem} (\textbf{Hoffman's bound})
		Let $\mu_1,\mu_n$ be the largest and smallest eigenvalues of the adjacency matrix of a graph $G$. The chromatic number of $G$ satisfies 
		\begin{align*}
		\chi_G \ge \frac{\mu_1 - \mu_n}{-\mu_n} = 1 + \frac{\mu_1}{-mu_n}.
		\end{align*}
	\end{theorem}
	
	
	
	
\end{document}
